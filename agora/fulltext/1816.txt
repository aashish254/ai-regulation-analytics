Guidance on Algorithmic Discrimination and the New Jersey Law Against Discrimination 

The New Jersey Office of the Attorney General and the Division on Civil Rights (DCR) issue this guidance to clarify how the New Jersey Law Against Discrimination (LAD) applies to algorithmic discrimination resulting from the use of new and emerging data-driven technologies, such as artificial intelligence (AI), by employers, housing providers, places of public accommodation, and other entities covered by the LAD.

[Opening recitals and factual discussion omitted.]

II. THE LAD PROHIBITS ALGORITHMIC DISCRIMINATION

The LAD applies to discrimination stemming from the use of automated decision-making tools in the same way it has long applied to other forms of discriminatory conduct. The LAD prohibits all forms of discrimination, irrespective of whether discriminatory conduct is facilitated by automated decision-making tools or driven by purely human practices. Indeed, consistent with its broad remedial purpose of eliminating discrimination in New Jersey, the LAD draws no distinctions based on the mechanism of discrimination. Thus, a covered entity—that is, an employer, housing provider, place of public accommodation, credit provider, contractor, or any other party subject to the LAD’s requirements—is not immunized from liability for violating the LAD merely because its discriminatory policy or practice involves using or relying on an automated decision-making tool. A covered entity can violate the LAD even if it has no intent to discriminate, and even if a third-party was responsible for developing the automated decision making tool. In short, claims of algorithmic discrimination are assessed consistent with other claims of discrimination under the LAD.

The LAD prohibits algorithmic discrimination on the basis of actual or perceived race, religion, color, national origin, sexual orientation, pregnancy, breastfeeding, sex, gender identity, gender expression, disability, and other protected characteristics. When employers, housing providers, places of public accommodation, or other covered entities use automated decision-making tools, they may violate the LAD if those tools result in disparate treatment based on a protected characteristic or if those tools have a disparate impact based on a protected characteristic. The LAD also prohibits algorithmic discrimination when it precludes or impedes the provision of reasonable accommodations, or of modifications to policies, procedures, or physical structures to ensure accessibility for people based on their disability, religion, pregnancy, or breastfeeding status.

Disparate Treatment Discrimination

Disparate treatment discrimination involves conduct by a covered entity that treats a person differently because of their membership in an LAD-protected class. Disparate treatment discrimination occurs if a policy or practice is intentionally discriminatory, or if a policy or practice is discriminatory on its face, even if a covered entity does not intend to discriminate. With respect to automated decision-making tools, a covered entity engages in disparate treatment discrimination when it designs or uses automated decision-making tools to treat members of a protected class differently. For example, housing providers engage in disparate treatment discrimination based on source of lawful income, an LAD-protected characteristic, if they use automated decision-making tools designed to exclude prospective tenants who use housing vouchers.

Disparate treatment discrimination may also occur if covered entities selectively use automated decision-making tools to assess members of a protected class—for example, if a housing provider uses a tenant screening algorithm only to evaluate Black prospective tenants but not prospective tenants of other races. Further, the use of an automated decision-making tool may result in disparate treatment discrimination even if the tool does not directly consider a protected characteristic but makes recommendations based on a close proxy for a protected characteristic. For example, a housing provider that uses a housing screening tool designed to select applicants who provide individual taxpayer identification numbers instead of Social Security numbers—for instance, because the housing provider wishes to rent only to immigrants based on their belief that immigrants will be less likely to challenge deficiencies in a property—discriminates based on national origin. Even if the tool does not explicitly consider national origin, an individual taxpayer identification number, which is only available to certain immigrants, is so closely associated with national origin that using it to screen applicants would in effect screen applicants based on national origin.

Disparate Impact Discrimination

Even if automated decision-making tools do not expressly assess people differently based on a protected characteristic, the use of these tools may violate the LAD if the use has a disproportionately negative effect on members of an LAD-protected class. This form of discrimination is known as disparate impact discrimination. 

Disparate impact discrimination occurs when policies or practices disproportionately affect members of an LAD-protected class in an unlawful manner. Specifically, policies and practices that result in a disparate impact are prohibited under the LAD unless they are necessary to achieve a substantial, legitimate, nondiscriminatory interest and there is no less discriminatory alternative that would achieve the same interest.41 This is true even if policies and practices are not discriminatory on their face—in other words, even if they are facially neutral—and are not motivated by discriminatory intent. Algorithmic discrimination constitutes disparate impact discrimination when an automated decision-making tool makes recommendations or contributes to decisions that disproportionately harm members of an LAD-protected class unless use of the tool serves a substantial, legitimate, nondiscriminatory interest. Even then, the use of the tool is prohibited if there is a less discriminatory alternative. In evaluating whether there are less discriminatory alternatives, whether the covered entity tested its automated decision-making tool for bias or evaluated alternatives may be considered as relevant evidence.

Disparate impact discrimination may arise, for example, if a company uses an automated decision making tool to assess contract bids that disproportionately screens out bids from women-owned businesses. Likewise, disparate impact discrimination may occur if a store uses facial recognition technology to ban former shoplifters that disproportionately generates false positives for patrons who wear religious headwear compared to other patrons. 

Reasonable Accommodations

Algorithmic discrimination may also violate the LAD if the use of automated decision-making tools precludes or impedes the provision of reasonable accommodations. This includes, but is not limited to, failing to make reasonable accommodations for a person’s disability, religion, pregnancy, or breastfeeding. The LAD requires a covered entity to make reasonable accommodations based on a person’s membership in a protected class if the entity knew or should have known about the need for an accommodation and the accommodation will not cause the entity an undue hardship.

A covered entity’s use of an automated decision-making tool may implicate reasonable accommodations in several ways. Reasonable accommodations may be necessary if an automated decision-making tool is inaccessible to individuals with disabilities or other individuals based on a protected characteristic. For example, reasonable accommodations may be necessary if an employer uses a tool to measure applicants’ typing speed that cannot measure typing from a non-traditional keyboard. Such a tool would not fairly or accurately assess the typing speed of an applicant who uses a non-traditional keyboard because of a disability.

Reasonable accommodations may also factor into recommendations by an automated decisionmaking tool that result in disparate impact discrimination. If an automated decision-making tool is not trained on data that includes individuals who use an accommodation, the tool may not recognize that an accommodation is possible or may penalize individuals who have or need a reasonable accommodation. If a covered entity acts on the recommendation made by such a tool, the covered entity may engage in disparate impact discrimination. When used in hiring, for example, such a tool may disproportionately exclude applicants who could perform the job with a reasonable accommodation. By accepting the tool’s recommendation to exclude these applicants, a covered entity could violate the LAD. Relatedly, some automated decision-making tools may fail to account for an existing reasonable accommodation. For example, if an employer uses a tool to monitor and track the productivity of its employees and the tool is programmed to flag atypical or unsanctioned breaks but is not programmed to consider reasonable accommodations, the tool may disproportionately flag for discipline employees who are allowed additional break time to accommodate a disability or to accommodate milk expression. The employer may violate the LAD if it accepts the recommendation from the tool to discipline these employees.

Who Is Liable for Discrimination Under the LAD

A covered entity is liable for any of its policies or practices that result in discrimination in violation of the LAD. In many cases, employers, housing providers, places of public accommodation, creditors, and contractors use automated decision-making tools they did not help to develop. Often, these covered entities rely largely or entirely on a third-party developer to create or implement the automated decision-making tool. Nevertheless, a covered entity is not shielded from liability for algorithmic discrimination that results from the entity’s use of an automated decision-making tool simply because the tool was developed by a third party or because the entity does not understand the inner workings of the tool.

It is critical that employers, housing providers, places of public accommodation, and other covered entities—as well as the developers and vendors of automated decision-making tools used by these entities—carefully consider and evaluate the design and testing of automated decision-making tools before they are deployed, and carefully analyze and evaluate those tools on an ongoing basis after they are deployed. Doing so is necessary to decrease the risk of discriminatory outcomes and thereby decrease the risk of possible liability under the LAD.

					***
					
The use of automated decision-making tools will continue to expand in key aspects of New Jerseyans’ lives. As it does, the risk of algorithmic discrimination may also increase. The LAD prohibits algorithmic discrimination and protects New Jerseyans from discrimination no matter the type of technology covered entities use. 

To find out more about the protections the LAD provides, go to www.NJCivilRights.gov. DCR also has several resources that provide an overview of the LAD. For more about algorithmic discrimination under the LAD, visit DCR’s website.

If you believe you have faced discrimination in violation of the LAD, you can file a complaint with DCR by visiting www.bias.njcivilrights.gov or calling 1-833-NJDCR4U (833-653-2748).

Sundeep Iyer
Director, New Jersey Division on Civil Rights
January 2025 
